# åŸæ¶æ„å›¾ä¸å®é™…ä»£ç çš„å·®å¼‚åˆ†æ

## âŒ åŸå›¾å­˜åœ¨çš„é—®é¢˜

### 1. **Encoder Blocks ä¸å®Œæ•´**
**åŸå›¾é—®é¢˜**ï¼š
- Level 1: åªç”»äº† ReducedAttn + FFW
- Level 3: åªç”»äº† ChannelAttn + GFFW

**å®é™…ä»£ç **ï¼š
```python
Enc_blocks: [2, 6, 10]  # æ¯å±‚æœ‰å¤šä¸ªblocks
```
- Level 1: 2ä¸ªblocksï¼Œæ¯ä¸ªblock: LayerNorm â†’ ReducedAttn â†’ LayerNorm â†’ FFW
- Level 2: 6ä¸ªblocks
- Level 3: 10ä¸ªblocksï¼Œä½¿ç”¨ ChannelAttn + GFFW

**åº”è¯¥ç”»æˆ**ï¼šæ˜¾ç¤ºå¤šä¸ªè¿ç»­çš„blocksï¼Œä¸æ˜¯åªç”»ä¸€ä¸ª

---

### 2. **Middle Block ä¸¥é‡é”™è¯¯** âš ï¸
**åŸå›¾é—®é¢˜**ï¼š
- åªç”»äº† "Fusion" æ¨¡å—
- æ²¡æœ‰ä½“ç°FHR (Frame History Router)
- æ²¡æœ‰æ˜¾ç¤ºæœ‰11ä¸ªblocks

**å®é™…ä»£ç **ï¼š
```python
Middle_blocks: 11
latent_attn_type1: "FHR"
latent_attn_type2: "Channel"
latent_attn_type3: "FHR"
```

**Middle Blockå®é™…åŒ…å«**ï¼š
1. **11ä¸ªTransformer blocks**
2. ä½¿ç”¨ **FHR + Channel Attention** ç»„åˆ
3. FHRä¼šç¼“å­˜ k/v ç”¨äºæ—¶åºä¿¡æ¯èšåˆ

**åº”è¯¥ç”»æˆ**ï¼š
```
[LayerNorm â†’ FHR/Channel â†’ LayerNorm â†’ GFFW] Ã— 11 blocks
```

---

### 3. **Decoder Block ä¸å‡†ç¡®**
**åŸå›¾é—®é¢˜**ï¼š
- ç”»çš„æ˜¯: Trans â†’ FGMA â†’ Fusion
- æ²¡æœ‰æ˜ç¡®æ˜¾ç¤ºFGMAå¦‚ä½•å·¥ä½œ

**å®é™…ä»£ç æµç¨‹**ï¼š
```python
decoder_attn_type2: "FGMA"  # æœ€åä¸€ä¸ªblockç”¨FGMA
```

**Decoderå®é™…æµç¨‹**ï¼š
1. å‰é¢çš„blocks: ChannelAttn
2. **æœ€åä¸€ä¸ªblock**: FGMA (Flow-Guided Masked Attention)
   - FGMAè¾“å…¥: å½“å‰å¸§ç‰¹å¾ + é‚»å¸§ç‰¹å¾
   - FGMAè¾“å‡º: Concat[Warped_feat, Mask, Attention_feat]
3. ç„¶åå¯èƒ½æœ‰FHRèåˆå¤šå¸§ä¿¡æ¯

**åº”è¯¥ç”»æˆ**ï¼š
```
Decoder Level N:
â”œâ”€ Block 1-9: [LayerNorm â†’ ChannelAttn â†’ LayerNorm â†’ GFFW]
â””â”€ Block 10 (last): [FGMA Alignment] â†’ [FHR Fusion]
```

---

### 4. **ç¼ºå°‘åŒç¼–ç å™¨** âš ï¸
**åŸå›¾é—®é¢˜**ï¼š
- å®Œå…¨æ²¡æœ‰ç”»åŒç¼–ç å™¨

**å®é™…ä»£ç **ï¼š
```python
use_dual_encoder: True
self.long_exposure_projection = nn.Conv2d(...)
self.short_exposure_projection = nn.Conv2d(...)
```

**åº”è¯¥åœ¨æœ€å¼€å§‹ç”»**ï¼š
```
Input (5 frames: T-2,T-1,T,T+1,T+2)
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Dual Encoderâ”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Long Exp    â”‚ â† For frames with blur
    â”‚ Short Exp   â”‚ â† For frames with noise
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Encoder Level 1...
```

---

### 5. **FGMAæ¨¡å—ç”»å¾—å¤ªç®€å•**
**åŸå›¾é—®é¢˜**ï¼š
- FGMAåªæ˜¯ä¸€ä¸ªè“è‰²æ–¹å—
- æ²¡æœ‰å±•ç¤ºå†…éƒ¨å·¥ä½œåŸç†

**FGMAå®é™…åŒ…å«ï¼ˆè®ºæ–‡æ ¸å¿ƒåˆ›æ–°ï¼‰**ï¼š
```python
1. SPyNet â†’ è®¡ç®—åŒå‘å…‰æµ (forward & backward)
2. Forward-Backward Consistency Check â†’ ç”Ÿæˆ Mask
   - D(i,j) = |L_{tâ†’t-1â†’t} - L_t|
   - M(i,j) = 1 if sÂ·D(i,j)/255 > 0.5 else 0
3. Reliable regions (M=0) â†’ ä½¿ç”¨å…‰æµwarp
4. Unreliable regions (M=1) â†’ ä½¿ç”¨sparse attention
5. Output: Concat[F_warped, M, F_attention]
```

**åº”è¯¥ç”»æˆè¯¦ç»†çš„æµç¨‹å›¾**ï¼š
```
Current Frame â”€â”€â”¬â”€â”€â†’ SPyNet â”€â”€â†’ F-B Check â”€â”€â†’ Mask M
                â”‚                                 â†“
Ref Frame â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â†’ Flow Warp (M=0)
                                 â”‚              â†“
                                 â””â”€â”€â†’ Attention (M=1)
                                                 â†“
                                            Concat
```

---

### 6. **Skip Connections æ²¡ç”»æ¸…æ¥š**
**åŸå›¾é—®é¢˜**ï¼š
- æ²¡æœ‰æ˜ç¡®æ ‡æ³¨skip connections

**å®é™…ä»£ç **ï¼š
```python
# Decoder Level 3
inp_dec_level3 = torch.cat([inp_dec_level3, out_enc_level3], 1)
inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)
```

**åº”è¯¥ç”»æˆ**ï¼š
- Encoder Level 1 â”€â”€â”€â”€â”€â†’ Decoder Level 1 (skip)
- Encoder Level 2 â”€â”€â”€â”€â”€â†’ Decoder Level 2 (skip)
- Encoder Level 3 â”€â”€â”€â”€â”€â†’ Decoder Level 3 (skip)

---

## âœ… æ­£ç¡®çš„æ¶æ„åº”è¯¥åŒ…å«

### æ•´ä½“æµç¨‹ï¼š
```
Input (5 frames)
    â†“
[Dual Encoder] (Long/Short)
    â†“
[Encoder L1: 2 blocks] â”€â”€skipâ”€â”€â”
    â†“ (downsample)              â”‚
[Encoder L2: 6 blocks] â”€â”€skipâ”€â”€â”¤
    â†“ (downsample)              â”‚
[Encoder L3: 10 blocks] â”€skipâ”€â”€â”¤
    â†“ (downsample)              â”‚
[Middle: 11 blocks with FHR]    â”‚
    â†“ (upsample)                â”‚
[Decoder L3: 10 blocks + FGMA] â†â”˜
    â†“ (upsample)                â”‚
[Decoder L2: 6 blocks + FGMA] â†â”€â”˜
    â†“ (upsample)                â”‚
[Decoder L1: 2 blocks + FGMA] â†â”€â”˜
    â†“
[Refinement: 2 blocks]
    â†“
Output (HDR)
```

### å…³é”®é…ç½®ï¼š
```yaml
Enc_blocks: [2, 6, 10]
Middle_blocks: 11
Dec_blocks: [10, 6, 2]
num_refinement_blocks: 2

# Encoder
encoder1: ReducedAttn + FFW
encoder2: ReducedAttn + FFW  
encoder3: ChannelAttn + GFFW

# Middle
latent: FHR + Channel + GFFW

# Decoder (æ¯å±‚æœ€åä¸€ä¸ªblockç”¨FGMA)
decoder1/2/3: Channel + FGMA + GFFW

# Refinement
refinement: ReducedAttn + GFFW
```

---

## ğŸ“‹ å»ºè®®

1. **é‡æ–°ç»˜åˆ¶æ•´ä½“æ¶æ„å›¾**ï¼ŒåŒ…å«ï¼š
   - åŒç¼–ç å™¨
   - å®Œæ•´çš„blockæ•°é‡
   - Skip connections
   - Middle blockçš„FHR

2. **è¯¦ç»†ç»˜åˆ¶FGMAæ¨¡å—å›¾**ï¼Œå±•ç¤ºï¼š
   - SPyNetå…‰æµä¼°è®¡
   - Forward-backward consistency check
   - Binary maskç”Ÿæˆ (Eq. 5)
   - Sparse attentionåªåœ¨maskåŒºåŸŸè®¡ç®—
   - æœ€ç»ˆConcatè¾“å‡º

3. **æ·»åŠ è®­ç»ƒç­–ç•¥å›¾**ï¼Œè¯´æ˜ï¼š
   - 30% optical flow (s=0)
   - 30% attention (s=âˆ)
   - 40% FGMA (séšæœºé‡‡æ ·)

4. **æ ‡æ³¨å…³é”®å‚æ•°**ï¼š
   - Sensitivity parameter s
   - Frame caching (T-2,T-1,T+1,T+2)
   - å„å±‚çš„é€šé“æ•°å˜åŒ–

---

## ğŸ¨ å¦‚ä½•è¿è¡Œç”Ÿæˆè„šæœ¬

æˆ‘å·²ç»åˆ›å»ºäº†æ­£ç¡®çš„ç»˜å›¾è„šæœ¬ï¼š`generate_correct_diagram.py`

è¿è¡Œæ–¹å¼ï¼š
```bash
cd h:\zzlzsh\Turtlenew
python generate_correct_diagram.py
```

ä¼šç”ŸæˆåŒ…å«3ä¸ªå­å›¾çš„å®Œæ•´æ¶æ„å›¾ï¼š
1. æ•´ä½“æ¶æ„ (å·¦ä¸Š+å³ä¸Š)
2. FGMAè¯¦ç»†æµç¨‹ (å·¦ä¸‹)
3. è®­ç»ƒç­–ç•¥ (å³ä¸‹)
